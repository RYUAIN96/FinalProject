{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 목표 : 머신러닝 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 직접 분류한 리뷰로 머신러닝 돌리기 => 정확도 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_1 = pd.read_csv(\"./data/sentiment_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_1 = sentiment_1.drop([\"Unnamed: 0\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>P/N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We’d never had Korean before and I’d been want...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I really was unsure of how much of the menu wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Absolutely delicious authentic Korean food ser...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  P/N\n",
       "0  We’d never had Korean before and I’d been want...    1\n",
       "1  I really was unsure of how much of the menu wo...    1\n",
       "2  Absolutely delicious authentic Korean food ser...    1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (1.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "# 전처리 작업을 위해 호출될 함수\n",
    "def preprocessor(text) :\n",
    "    # 문자열의 내의 html 태그를 삭제한다.\n",
    "    # 문자열에서 이모티콘을 찾아낸다.\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)|\\^.?\\^', str(text))\n",
    "    # 문장에서 특수문자를 제거하고\n",
    "    # 문자열을 소문자로 변하고\n",
    "    # 추출한 이모티콘을 붙혀준다.\n",
    "    text = re.sub('[\\W]+', ' ', str(text).lower() + ' '.join(emoticons).replace('-', ''))\n",
    "    # print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentiment_1[\"review\"] = sentiment_1[\"review\"].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_1.to_csv(\"./data/refined_review.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>P/N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we d never had korean before and i d been want...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i really was unsure of how much of the menu wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>absolutely delicious authentic korean food ser...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the banchan or side dishes that they serve are...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my eleven year old twins beg to get biminbop o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  P/N\n",
       "0  we d never had korean before and i d been want...    1\n",
       "1  i really was unsure of how much of the menu wo...    1\n",
       "2  absolutely delicious authentic korean food ser...    1\n",
       "3  the banchan or side dishes that they serve are...    1\n",
       "4  my eleven year old twins beg to get biminbop o...    1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refined_review = pd.read_csv(\"./data/refined_review.csv\")\n",
    "refined_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def step2_preprocessing() :\n",
    "#     # csv 데이터를 읽어온다.\n",
    "#     df = pd.read_csv('./data/trip_final.csv')\n",
    "\n",
    "#     # 전처리 작업\n",
    "#     stime = time()\n",
    "#     print('전처리 시작')\n",
    "#     df[\"review\"] = df['review'].apply(preprocessor)\n",
    "#     print('전처리 완료')\n",
    "#     print('소요시간 : %d' % (time() - stime))\n",
    "\n",
    "#     # 전처리된 데이터를 저장한다.\n",
    "#     df.to_csv('./data/pre_review.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평점 전처리\n",
    "def star_proprocessing(text) :\n",
    "    value = int(text)\n",
    "    if value <= 3.0 :\n",
    "        return '0'\n",
    "    else :\n",
    "        return '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step2_preprocessing():\n",
    "    # 수집한 데이터를 읽어온다.\n",
    "    df = pd.read_csv('./data/trip_final.csv')\n",
    "    # print(df)\n",
    "\n",
    "    # 전처리 과정\n",
    "    df['rating'] = df['rating'].apply(star_proprocessing)\n",
    "    # 학습 데이터와 테스트 데이터로 나눈다.\n",
    "    text_list = df['review'].tolist()\n",
    "    star_list = df['rating'].tolist()\n",
    "\n",
    "    text_train, text_test, star_train, star_test = train_test_split(text_list, star_list, test_size=0.3, random_state=0)\n",
    "    #print(len(text_train))\n",
    "    #print(len(text_test))\n",
    "    #print(len(star_train))\n",
    "    #print(len(star_test))\n",
    "\n",
    "    # 저장한다.\n",
    "    dic_train = {\n",
    "        'text' : text_train,\n",
    "        'star' : star_train\n",
    "    }\n",
    "    df_tran = pd.DataFrame(dic_train)\n",
    "\n",
    "    dic_test = {\n",
    "        'text' : text_test,\n",
    "        'star' : star_test\n",
    "    }\n",
    "    df_test = pd.DataFrame(dic_test)\n",
    "\n",
    "    df_tran.to_csv('./data/trip_train_data.csv', index=False)\n",
    "    df_test.to_csv('./data/trip_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'step2_preprocessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-e49d6ac9d758>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstep2_preprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'step2_preprocessing' is not defined"
     ]
    }
   ],
   "source": [
    "# step2_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# step3_word_tokenizer.py\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# stopword 단어 사전을 다운로드 받는다.\n",
    "nltk.download('stopwords')\n",
    "# stopword 데이터를 가져온다.\n",
    "stop = stopwords.words('english')\n",
    "# 단어 줄기를 하기위한 객체\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공백으로 단어분리\n",
    "def tokenizer(text) :\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어줄기\n",
    "def tokenizer_porter(text) :\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_stopwordsr(text) :\n",
    "    # 띄어쓰기를 기준으로 분리한다.\n",
    "    word_list = text.split()\n",
    "    #단어 줄기 처리\n",
    "    word_list2 = \\\n",
    "        [porter.stem(word) for word in word_list]\n",
    "    #불용어 처리\n",
    "    result = []\n",
    "    for w in word_list2: \n",
    "        if w not in stop: \n",
    "            result.append(w)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step3_word_tokenizer() :\n",
    "    text = 'runners like running and thus they run'\n",
    "\n",
    "    a1 = tokenizer(text)\n",
    "    a2 = tokenizer_porter(text)\n",
    "    print('a1 :', a1)\n",
    "    print('a2 :', a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step4_learning() :\n",
    "    # csv 파일에서 데이터를 읽어온다.\n",
    "    df = pd.read_csv('./data/refined_review.csv')\n",
    "    # 테스트, 학습데이터로 나눈다.\n",
    "    X_train = df.loc[:700 - 1, 'review'].values\n",
    "    y_train = df.loc[:700 - 1, 'P/N'].values\n",
    "\n",
    "    X_test = df.loc[300:, 'review'].values\n",
    "    y_test = df.loc[300:, 'P/N'].values\n",
    "\n",
    "    # 단어장을 만들어주는 객체 생성\n",
    "    tfidf = TfidfVectorizer(lowercase=False, tokenizer=tokenizer)\n",
    "    # tfidf = TfidfVectorizer(lowercase=False, tokenizer=tokenizer_stopwordsr)\n",
    "    # 데이터를 학습하기 위한 객체\n",
    "    logistic = LogisticRegression(C=10.0, penalty='l2', random_state=0)\n",
    "    # 파이프 라인 설정\n",
    "    pipeline = Pipeline([('vect', tfidf), ('clf', logistic)])\n",
    "\n",
    "    # 학습한다.\n",
    "    stime = time()\n",
    "    print('학습 시작')\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    print('학습 종료')\n",
    "    print('총 학습시간 : %d' % (time() - stime))\n",
    "\n",
    "    # 테스트\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    print(\"정확도 : %.3f\" % accuracy_score(y_test, y_pred))\n",
    "\n",
    "    # 성능 확인\n",
    "    y_true = y_test\n",
    "    y_hat = y_pred\n",
    "    print(\"R2 score : \", r2_score(y_true, y_hat))\n",
    "    print(\"mean_absolute_error : \", mean_absolute_error(y_true, y_hat))\n",
    "    print(\"mean_squared_error : \", mean_squared_error(y_true, y_hat))\n",
    "\n",
    "    # 학습이 완료된 객체를 저장한다.\n",
    "    with open('./data/trip.dat', 'wb') as fp :\n",
    "        pickle.dump(pipeline, fp)\n",
    "\n",
    "    print('저장완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 시작\n",
      "학습 종료\n",
      "총 학습시간 : 0\n",
      "정확도 : 0.951\n",
      "R2 score :  0.4861405405405407\n",
      "mean_absolute_error :  0.04864091559370529\n",
      "mean_squared_error :  0.04864091559370529\n",
      "저장완료\n"
     ]
    }
   ],
   "source": [
    "step4_learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 별점으로 긍정부정 머신러닝 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_2 = pd.read_csv(\"./data/sentiment_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_2 = sentiment_2.drop([\"Unnamed: 0\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_2[\"review\"] = sentiment_2[\"review\"].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_2.to_csv(\"./data/refined_review_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'refined_review_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-b0c534c0bd79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrefined_review_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrefined_review_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'refined_review_2' is not defined"
     ]
    }
   ],
   "source": [
    "refined_review_2 = refined_review_2.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_review_2.to_csv(\"./data/refined_review_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "refined_review_2 = pd.read_csv(\"./data/refined_review_2.csv\")\n",
    "refined_review_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "refined_review_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_review_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step4_learning() :\n",
    "    # csv 파일에서 데이터를 읽어온다.\n",
    "    df = pd.read_csv('./data/refined_review_2.csv')\n",
    "    # 테스트, 학습데이터로 나눈다.\n",
    "    X_train = df.loc[:35000, 'review'].values\n",
    "    y_train = df.loc[:35000, 'P/N'].values\n",
    "\n",
    "    X_test = df.loc[15000:, 'review'].values\n",
    "    y_test = df.loc[15000:, 'P/N'].values\n",
    "\n",
    "    # 단어장을 만들어주는 객체 생성\n",
    "    tfidf = TfidfVectorizer(lowercase=False, tokenizer=tokenizer)\n",
    "    # tfidf = TfidfVectorizer(lowercase=False, tokenizer=tokenizer_stopwordsr)\n",
    "    # 데이터를 학습하기 위한 객체\n",
    "    logistic = LogisticRegression(C=10.0, penalty='l2', random_state=0)\n",
    "    # 파이프 라인 설정\n",
    "    pipeline = Pipeline([('vect', tfidf), ('clf', logistic)])\n",
    "\n",
    "    # 학습한다.\n",
    "    stime = time()\n",
    "    print('학습 시작')\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    print('학습 종료')\n",
    "    print('총 학습시간 : %d' % (time() - stime))\n",
    "\n",
    "    # 테스트\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    print(\"정확도 : %.3f\" % accuracy_score(y_test, y_pred))\n",
    "\n",
    "    # 성능 확인\n",
    "    y_true = y_test\n",
    "    y_hat = y_pred\n",
    "    print(\"R2 score : \", r2_score(y_true, y_hat))\n",
    "    print(\"mean_absolute_error : \", mean_absolute_error(y_true, y_hat))\n",
    "    print(\"mean_squared_error : \", mean_squared_error(y_true, y_hat))\n",
    "\n",
    "    # 학습이 완료된 객체를 저장한다.\n",
    "    with open('./data/trip_2.dat', 'wb') as fp :\n",
    "        pickle.dump(pipeline, fp)\n",
    "\n",
    "    print('저장완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 시작\n",
      "학습 종료\n",
      "총 학습시간 : 0\n",
      "정확도 : 0.951\n",
      "R2 score :  0.4861405405405407\n",
      "mean_absolute_error :  0.04864091559370529\n",
      "mean_squared_error :  0.04864091559370529\n",
      "저장완료\n"
     ]
    }
   ],
   "source": [
    "step4_learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 직접 분류한 데이터셋 2000개 가지고 학습 시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sent = pd.read_csv(\"./data/final_sent.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sent = final_sent.drop([\"Unnamed: 0\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sent[\"review\"] = final_sent[\"review\"].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sent.to_csv(\"./data/refined_final_sent.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_final_sent = refined_final_sent.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_final_sent.to_csv(\"./data/refined_final_sent.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>PN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>our first dining experience in a north korean ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it was my first meal in north korean restauran...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>food are generally good tried lunch bulkogi gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>food is generally good worth mentioning are ki...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>we ve passed by the place a couple times and w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  PN\n",
       "0  our first dining experience in a north korean ...   1\n",
       "1  it was my first meal in north korean restauran...   1\n",
       "2  food are generally good tried lunch bulkogi gr...   1\n",
       "3  food is generally good worth mentioning are ki...   1\n",
       "4  we ve passed by the place a couple times and w...   1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refined_final_sent = pd.read_csv(\"./data/refined_final_sent.csv\")\n",
    "refined_final_sent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review    0\n",
       "PN        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refined_final_sent.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step4_learning() :\n",
    "    # csv 파일에서 데이터를 읽어온다.\n",
    "    df = pd.read_csv('./data/refined_final_sent.csv')\n",
    "    # 테스트, 학습데이터로 나눈다.\n",
    "    X_train = df.loc[:1400, 'review'].values\n",
    "    y_train = df.loc[:1400, 'PN'].values\n",
    "\n",
    "    X_test = df.loc[700:, 'review'].values\n",
    "    y_test = df.loc[700:, 'PN'].values\n",
    "\n",
    "    # 단어장을 만들어주는 객체 생성\n",
    "    tfidf = TfidfVectorizer(lowercase=False, tokenizer=tokenizer)\n",
    "    # tfidf = TfidfVectorizer(lowercase=False, tokenizer=tokenizer_stopwordsr)\n",
    "    # 데이터를 학습하기 위한 객체\n",
    "    logistic = LogisticRegression(C=10.0, penalty='l2', random_state=0)\n",
    "    # 파이프 라인 설정\n",
    "    pipeline = Pipeline([('vect', tfidf), ('clf', logistic)])\n",
    "\n",
    "    # 학습한다.\n",
    "    stime = time()\n",
    "    print('학습 시작')\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    print('학습 종료')\n",
    "    print('총 학습시간 : %d' % (time() - stime))\n",
    "\n",
    "    # 테스트\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    print(\"정확도 : %.3f\" % accuracy_score(y_test, y_pred))\n",
    "\n",
    "    # 성능 확인\n",
    "    y_true = y_test\n",
    "    y_hat = y_pred\n",
    "    print(\"R2 score : \", r2_score(y_true, y_hat))\n",
    "    print(\"mean_absolute_error : \", mean_absolute_error(y_true, y_hat))\n",
    "    print(\"mean_squared_error : \", mean_squared_error(y_true, y_hat))\n",
    "\n",
    "    # 학습이 완료된 객체를 저장한다.\n",
    "    with open('./data/trip_3.dat', 'wb') as fp :\n",
    "        pickle.dump(pipeline, fp)\n",
    "\n",
    "    print('저장완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 시작\n",
      "학습 종료\n",
      "총 학습시간 : 0\n",
      "정확도 : 0.956\n",
      "R2 score :  0.5979720918101199\n",
      "mean_absolute_error :  0.04391371340523883\n",
      "mean_squared_error :  0.04391371340523883\n",
      "저장완료\n"
     ]
    }
   ],
   "source": [
    "step4_learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 직접 만든 stopward로 머신러닝 돌려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['about','also', 'and','because','box','dish','dishes','etc','front','cashier', \"husband\", 'immediately','just','menu', 'minutes','others',\n",
    "'our','relay','section', 'some', 'that', 'the', 'their','them', 'then', 'this', 'very', 'walked', 'was', 'were','what', 'when', 'women','busy','gotta','been','here', 'times', 'and', 'this','the','are',\n",
    "'there','options','had','have','you','find','authentic','pretty','choose',\n",
    "'was','parking ','were','plenty','pot','dishes','they','serve','free','low',\n",
    "'day','cuz','eat','box','today','give','chance','something','else','really','short',\n",
    "'wife','plate','always','second','time','very','came','out','thier',\n",
    "'bowl','table','still','bottom','sunny','all','add','some','your','kept','almost',\n",
    "'definitly','friend','recently','stopped','arrived','packed','long','plus','including',\n",
    "'cellophane','slivers','went','night','staying','when','home','wide','now','minutes',\n",
    "'them','that','other','just','their','range','once','car','drive','miles','wanted',\n",
    "'stop','along','would','waiting','stay','land','people','brought','half',\n",
    "'dozen','mixes','each','yeas','locations','found','especially','significantly','plae',\n",
    "'two','blocks','away','trained','places','quite','can','min','years','started','lived',\n",
    "'countries','myself','versed','twice','week','owned','own','currently','has','sushi',\n",
    "'order','correctly','around','couple','sooo','through','sister',\n",
    "'rest','being','three','ordered','huge','yakimondo','say','portions','large','groups',\n",
    "'lover','due','neighbor','makes','most','pass','only','looking','word','mouth','building'\n",
    "'outside','interior','where','welcome','front','opted','guest','another','presentation',\n",
    "'larger','show','his','work','done','mizo','start','while','did','sample',\n",
    "'till','open','seats','across','street','which','certainly','proclaim','thought',\n",
    "'somewhat','amount','navigating','hole','wall','gets','anything','also','ask','how',\n",
    "'into','probably','put','anyway','yesterday','honestly','whole','its','speak',\n",
    "'corner','sick','ethnic','everything','coming','kitchen','things','defrosted',\n",
    "'properly','black','below','average','frozen','let','down','definitely','extremely',\n",
    "'what','randomly','during','our','managed','past','location','exterior','lot',\n",
    "'alone','single','piece','gimchi''told','whether','ended','takeaway','aspects',\n",
    "'dropped','accumulated','mother','law','completely','aged','finding','round','fooled',\n",
    "'highly','explain','those','who','tourist','itself','themed','decorations','seems',\n",
    "'delicately','ranges','udon','persons','sets','fix','basement','any','turning',\n",
    "'reservation','literally','meters','tram','station','plain','view','look',\n",
    "'fact','stairs','near','helping','after','step','realize','setting','pleasing',\n",
    "'decor','motives','modern','lines','entered','certify','personal','touch','noticed',\n",
    "'high','chair','return','weekend','per','noticing','accommodation','book','advance',\n",
    "'evening','upon','arrival','considered','ourselves','tables','predominantly', 'variations',\n",
    "'boyfriend','clear','even','joked','asking','prompt','center','longer',\n",
    "'morning','sight','seeing','courteous','overpaying','gave','mispronounced',\n",
    "'thoroughly','trio','cokes','walking','dad','took','mums','house','might','comfort',\n",
    "'mean','isn','either','reminded','higher','because','prime','hours','search','gem',\n",
    "'required','minute','ride','see','neighborhood','part','flat','city','accept',\n",
    "'carry','imagine','ready','known','customer','accommodated','requests','daughter',\n",
    "'owner','patient','help','split','types','non','pesto','beginning','choices',\n",
    "'end','saved','tongues','every','pay','sized','portion','background','playing',\n",
    "'person','starter', 'mension','sayd','does','locate','environment',\n",
    "'true','stated','splendid','she','sorry','expecting','layers','realized',\n",
    "'redo','both','leaves','slightly','broth','standards','serving','recommending',\n",
    "'prepared','presented','characteristics','personally','her','meant','immediately',\n",
    "'much','chat','woman','card','actually','path','means','dress','before','line',\n",
    "'since','hair','real','speaks','simple','centrally','located','english',\n",
    "'issue','owners','afternoon','four','warm','biggest','cozy','making','meet',\n",
    "'enter','definetly','semi','gone','particularly','available',\n",
    "'walk','downside','crowns','rooms','feels','totally','knew', 'guess',\n",
    "'advisable','ahead','incredibly','apartment','advisor','heart','warming','door',\n",
    "'notch','area','somewhere','boxes','absolutely','frequent','eater','reference',\n",
    "'material','unassuming','arrive','nonetheless','cash','earth','abroad','exquisite',\n",
    "'reason','conditioning','ventilation','mindblowing','mention','hold','sitting',\n",
    "'air','ground','customers','days','then','doing','tricky','road','avarage','definately',\n",
    "'give','soon','everyday','quite','get','cloths','hanger','cute','hidden','part',\n",
    "'gladly','for','czk','simply','starters','allowed','importantly','mom', 'uber','ride','walk',\n",
    "'our','group','comes','aunt','owns','grew','hands','school','absolute','having','seen',\n",
    "'steps','nothing','quiet','partner','tray','etc','searching','lie','finish','centre',\n",
    "'brought','everyone','older','usually', 'increased','evenings','booking','confirm',\n",
    "'authenticity','luck','planned','customers','kinds','stuff','centrum','going','smaller',\n",
    "'shop','overall','mix','already','much','past','month','birthday','celebration','terms',\n",
    "'delve','below','wise','attentiveness','empty','occassion','revealed','amplified',\n",
    "'nervous','guests','seated','anyone','hello','mins','happening','third','somehow',\n",
    "'crowd','youngsters','winter','jacket','live','nearby','substantially','improved',\n",
    "'jap','identify','blamed','favouring','provides','extreme','only','pictures',\n",
    "'study','frequently','think','eum','ppong','talking','tap','exactly','remembers',\n",
    "'visiting','placed','address','speaking','dull','humble','surroundings','cuz',\n",
    "'light','hospital','suchi','further','relatively','skeptical','chocked','par',\n",
    "'letdown','tad','bland','dry','why','prohibitively','either','washed','downplay',\n",
    "'extensive','thing','massive','rush','touched','job','growing','anymore','reservations',\n",
    "'keep','offering','reading','often','nonsense','number','etnic','extra','assurance',\n",
    "'decor','thus','maybe','romantic','date','yourself','gentleman','rate','five','perhaps',\n",
    "'amounts','suspicions','confirmed','sight','crystals','struggle','hungry','taxi',\n",
    "'man','knows','occasion','able','booked','dined','temperature','host','checked',\n",
    "'miss','four,','correct','fat','stole','gentle','town','plus','move','stops',\n",
    "'prepared','played','pop','spice','help','personnel','girlfriend','genuinly','version',\n",
    "'doubt','occasional','tailored','given','honest','aware','missed','moving','expectations',\n",
    "'key','easiest','zizkov','staircase','map','eventually','passion','othet','let',\n",
    "'plates','expecting','arugula','minced','manager','fifteen','complicated',\n",
    "'ours','stayed','hotel','relation','usual','unbeatable','appears','may','tonight',\n",
    "'europe','fluent','encountered','truly', 'accident','randomly','living','obsessed',\n",
    "'goes','cross','moved','became','lazy','whatever','responsive','regard','descriptive',\n",
    "'listings','maintaining','wooing','evr','tourism','greeted','months','traveling',\n",
    "'happened','world','recipes','cure','weakness','ower','warmth','afford','fuss',\n",
    "'forward','else','office','week','esp','item','crowns','via','returned',\n",
    "'design','discover']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
